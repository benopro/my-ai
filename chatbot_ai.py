import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ğŸ”¹ Chá»n mÃ´ hÃ¬nh AI (CÃ³ thá»ƒ thay báº±ng LLaMA-2 hoáº·c mÃ´ hÃ¬nh nháº¹ hÆ¡n náº¿u cáº§n)
MODEL_NAME = "microsoft/phi-4"  # Hoáº·c "meta-llama/Llama-2-7b-chat-hf"
USE_GPU = torch.cuda.is_available()  # Kiá»ƒm tra xem cÃ³ GPU khÃ´ng

# ğŸ”¹ Chá»n thiáº¿t bá»‹ phÃ¹ há»£p
device = "cuda" if USE_GPU else "cpu"
dtype = torch.float16 if USE_GPU else torch.float32  # CPU khÃ´ng há»— trá»£ float16

print(f"ğŸš€ Äang táº£i mÃ´ hÃ¬nh {MODEL_NAME} trÃªn {device.upper()}...")

# ğŸ”¹ Táº£i tokenizer vÃ  mÃ´ hÃ¬nh
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, 
    torch_dtype=dtype, 
    device_map=device
)

# ğŸ”¹ HÃ m cháº¡y chatbot
def chatbot(prompt):
    """Nháº­n Ä‘áº§u vÃ o tá»« ngÆ°á»i dÃ¹ng vÃ  sinh pháº£n há»“i tá»« AI."""
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# ğŸ”¹ Cháº¡y chatbot
if __name__ == "__main__":
    print("ğŸ¤– Chatbot AI - Nháº­p 'exit' Ä‘á»ƒ thoÃ¡t")
    while True:
        user_input = input("Báº¡n: ")
        if user_input.lower() == "exit":
            print("Táº¡m biá»‡t!")
            break
        response = chatbot(user_input)
        print("AI:", response)

